from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings, ChatGoogleGenerativeAI
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser
import os

# API í‚¤ ì„¤ì •
os.environ["GOOGLE_API_KEY"] = "AIzaSyDWeeAT3iJ1nUAk3UrX1LVeIMlVv2gpBV4"

# 1. ì €ì¥ëœ ì¸ë±ìŠ¤ ë¶ˆëŸ¬ì˜¤ê¸°
# rag-build-index.pyì™€ ë™ì¼í•œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

# 'allow_dangerous_deserialization=True'ëŠ” ì‹ ë¢°í•  ìˆ˜ ìˆëŠ” ë¡œì»¬ íŒŒì¼ì¼ ê²½ìš°ì—ë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.
new_db = FAISS.load_local(
    "my_company_index",
    embeddings,
    allow_dangerous_deserialization=True
)

# 2. ê²€ìƒ‰ê¸°(Retriever)ë¡œ ë³€í™˜
retriever = new_db.as_retriever(search_kwargs={"k": 2})

# 3. LLM ì„¤ì • (Gemini 2.0 Flash)
llm = ChatGoogleGenerativeAI(model="gemini-2.0-flash", temperature=0)

# 4. í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì •ì˜
prompt = ChatPromptTemplate.from_template("""
ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•´ì£¼ì„¸ìš”.
ë‹µë³€ì€ ì»¨í…ìŠ¤íŠ¸ì— ìˆëŠ” ì •ë³´ë§Œ ì‚¬ìš©í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸:
{context}

ì§ˆë¬¸: {question}

ë‹µë³€:
""")

# 5. ë¬¸ì„œ í¬ë§·íŒ… í•¨ìˆ˜
def format_docs(docs):
    print(docs)
    return "\n\n".join(doc.page_content for doc in docs)

# 6. LCEL ì²´ì¸ êµ¬ì„±
rag_chain = (
    {"context": retriever | format_docs, "question": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
)

# 7. ì§ˆë¬¸í•˜ê¸° í•¨ìˆ˜
def ask_question(query):
    print(f"\nğŸ™‹ ì§ˆë¬¸: {query}")
    result = rag_chain.invoke(query)
    print(f"ğŸ¤– ë‹µë³€: {result}")

# í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    ask_question("ì¬íƒê·¼ë¬´ ì‹ ì²­ì€ ëª‡ ì‹œê¹Œì§€ì•¼?")
    ask_question("ì›Œì¼€ì´ì…˜ ê·œì • ì•Œë ¤ì¤˜")