# !pip install google-cloud-aiplatform langchain langchain-google-genai langchain-google-vertexai langchain-community

from google.cloud import aiplatform
from langchain_google_genai import GoogleGenerativeAI, GoogleGenerativeAIEmbeddings
from langchain_google_vertexai import VectorSearchVectorStore
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_core.output_parsers import StrOutputParser

# ==========================================
# 1. í™˜ê²½ ì„¤ì •
# ==========================================
PROJECT_ID = "project-id"            # í”„ë¡œì íŠ¸ ID ì…ë ¥ (Console ëŒ€ì‹œë³´ë“œì—ì„œ í™•ì¸)
LOCATION = "us-central1"             # ë¦¬ì „ (ì„œìš¸ì€ asia-northeast3)
INDEX_ID = "your-index-id"           # Vertex AI Vector Search Index ID
ENDPOINT_ID = "your-endpoint-id"     # Vertex AI Vector Search Endpoint ID

# ==========================================
# 2. ì¸ì¦ ì„¤ì •
# ==========================================
import os

# Colab í™˜ê²½ ê°ì§€
try:
    import google.colab
    IN_COLAB = True
except ImportError:
    IN_COLAB = False

if IN_COLAB:
    print("ğŸ”§ Colab í™˜ê²½ ê°ì§€ - gcloud ì¸ì¦ ì‹œì‘")
    print("\në‹¤ìŒ ë‹¨ê³„ë¥¼ ë”°ë¼ì£¼ì„¸ìš”:")
    print("1. ì•„ë˜ ëª…ë ¹ì–´ ì‹¤í–‰ í›„ ë‚˜ì˜¤ëŠ” URLì„ ë¸Œë¼ìš°ì €ì—ì„œ ì—´ê¸°")
    print("2. Google Cloud í¬ë ˆë”§ ê³„ì •ìœ¼ë¡œ ë¡œê·¸ì¸")
    print("3. ì¸ì¦ ì½”ë“œë¥¼ ë³µì‚¬í•˜ì—¬ ì…ë ¥\n")

    # gcloud ì¸ì¦
    os.system("gcloud auth login --no-launch-browser")

    # í”„ë¡œì íŠ¸ ì„¤ì •
    os.system(f"gcloud config set project {PROJECT_ID}")

    # Application Default Credentials ì„¤ì •
    os.system("gcloud auth application-default login --no-launch-browser")

    print("\nâœ… Colab ì¸ì¦ ì™„ë£Œ")
else:
    # ë¡œì»¬ í™˜ê²½: í„°ë¯¸ë„ì—ì„œ í•œ ë²ˆë§Œ ì‹¤í–‰
    # $ gcloud auth application-default login
    print("ğŸ’» ë¡œì»¬ í™˜ê²½ - ADC ì‚¬ìš©")
    print("í„°ë¯¸ë„ì—ì„œ ë‹¤ìŒ ëª…ë ¹ì–´ë¥¼ ì‹¤í–‰í•˜ì„¸ìš”:")
    print("$ gcloud auth application-default login")

# í•„ìš”í•œ ê¶Œí•œ:
# - Vertex AI User (roles/aiplatform.user)
# - Storage Object Admin (roles/storage.objectAdmin) - GCS ì‚¬ìš© ì‹œ

# Vertex AI ì´ˆê¸°í™”
aiplatform.init(project=PROJECT_ID, location=LOCATION)

# ==========================================
# 3. LangChain ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
# ==========================================

# ì„ë² ë”© ëª¨ë¸ ì´ˆê¸°í™” (Google Generative AI Embeddings)
embeddings = GoogleGenerativeAIEmbeddings(
    model="text-embedding-004",  # ìµœì‹  ì„ë² ë”© ëª¨ë¸
    project=PROJECT_ID,
    location=LOCATION
)

# LLM ì´ˆê¸°í™” (Google Generative AI)
llm = GoogleGenerativeAI(
    model="gemini-1.5-pro",  # ë˜ëŠ” gemini-1.5-flash
    project=PROJECT_ID,
    location=LOCATION,
    temperature=0.2,
    max_output_tokens=1024,
)

# ==========================================
# 4. ë¬¸ì„œ ì¤€ë¹„ ë° Vector Store ì—°ê²°
# ==========================================

# ì˜ˆì œ ë¬¸ì„œ ë°ì´í„° (ì‹¤ì œë¡œëŠ” PDF, ì›¹ì‚¬ì´íŠ¸ ë“±ì—ì„œ ë¡œë“œ)
sample_texts = [
    "LGì´ë…¸í…ì€ ì „ìë¶€í’ˆ ì „ë¬¸ê¸°ì—…ì…ë‹ˆë‹¤. ì¹´ë©”ë¼ ëª¨ë“ˆ, ê¸°íŒ, ëª¨í„° ë“±ì„ ìƒì‚°í•©ë‹ˆë‹¤.",
    "LGì´ë…¸í…ì˜ ì£¼ìš” ì œí’ˆì€ ìŠ¤ë§ˆíŠ¸í°ìš© ì¹´ë©”ë¼ ëª¨ë“ˆì…ë‹ˆë‹¤.",
    "íšŒì‚¬ëŠ” 2008ë…„ LGì „ìì˜ ë¶€í’ˆ ì‚¬ì—…ë¶€ê°€ ë¶„ì‚¬í•˜ì—¬ ì„¤ë¦½ë˜ì—ˆìŠµë‹ˆë‹¤.",
    "ë³¸ì‚¬ëŠ” ì„œìš¸íŠ¹ë³„ì‹œ ì¤‘êµ¬ì— ìœ„ì¹˜í•˜ê³  ìˆìŠµë‹ˆë‹¤.",
    "LGì´ë…¸í…ì€ Appleì˜ ì£¼ìš” ì¹´ë©”ë¼ ëª¨ë“ˆ ê³µê¸‰ì—…ì²´ì…ë‹ˆë‹¤.",
]

# Document ê°ì²´ë¡œ ë³€í™˜
documents = [Document(page_content=text, metadata={"source": f"doc_{i}"})
             for i, text in enumerate(sample_texts)]

# í…ìŠ¤íŠ¸ ë¶„í•  (ê¸´ ë¬¸ì„œì˜ ê²½ìš°)
text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    length_function=len,
)
split_docs = text_splitter.split_documents(documents)

print(f"ğŸ“„ ì´ {len(split_docs)}ê°œì˜ ë¬¸ì„œ ì²­í¬ ì¤€ë¹„ ì™„ë£Œ")

# ==========================================
# 5. Vertex AI Vector Search ì—°ê²°
# ==========================================
# Option 1: ê¸°ì¡´ Index ì‚¬ìš© (ì´ë¯¸ ìƒì„±ëœ ê²½ìš°)
# INDEX_IDì™€ ENDPOINT_IDë¥¼ Consoleì—ì„œ í™•ì¸í•˜ì—¬ ì…ë ¥

vector_store = VectorSearchVectorStore.from_components(
    project_id=PROJECT_ID,
    region=LOCATION,
    index_id=INDEX_ID,
    endpoint_id=ENDPOINT_ID,
    embedding=embeddings,
)

print("âœ… Vector Store ì—°ê²° ì™„ë£Œ")

# ë¬¸ì„œë¥¼ Vector Storeì— ì¶”ê°€ (ìµœì´ˆ ì‹¤í–‰ ì‹œ ë˜ëŠ” ë¬¸ì„œ ì—…ë°ì´íŠ¸ ì‹œ)
# ì£¼ì„ì„ í•´ì œí•˜ì—¬ ì‚¬ìš©:
# texts = [doc.page_content for doc in split_docs]
# metadatas = [doc.metadata for doc in split_docs]
# vector_store.add_texts(texts=texts, metadatas=metadatas)
# print(f"âœ… {len(texts)}ê°œ ë¬¸ì„œë¥¼ Vector Storeì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤")

# ==========================================
# Option 2: ê¸°ì¡´ Indexì— ìƒˆë¡œìš´ ë¬¸ì„œ ì¶”ê°€
# ==========================================
# ì£¼ì˜: Indexì™€ Endpointê°€ ì´ë¯¸ ìƒì„±ë˜ì–´ ìˆì–´ì•¼ í•©ë‹ˆë‹¤
#
# # Vector Store ì´ˆê¸°í™”
# vector_store = VectorSearchVectorStore.from_components(
#     project_id=PROJECT_ID,
#     region=LOCATION,
#     index_id=INDEX_ID,
#     endpoint_id=ENDPOINT_ID,
#     embedding=embeddings,
# )
#
# # ë¬¸ì„œ ì¶”ê°€ (í…ìŠ¤íŠ¸ì™€ ë©”íƒ€ë°ì´í„° ë¶„ë¦¬)
# texts = [doc.page_content for doc in split_docs]
# metadatas = [doc.metadata for doc in split_docs]
#
# # Vector Storeì— ë¬¸ì„œ ì¶”ê°€
# vector_store.add_texts(texts=texts, metadatas=metadatas)
# print(f"âœ… {len(texts)}ê°œ ë¬¸ì„œë¥¼ Vector Storeì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤")

# ==========================================
# 6. RAG Chain êµ¬ì„± (LCEL ë°©ì‹)
# ==========================================

# Retriever ìƒì„± (ìœ ì‚¬ë„ ê²€ìƒ‰)
retriever = vector_store.as_retriever(
    search_type="similarity",
    search_kwargs={"k": 3}  # ìƒìœ„ 3ê°œ ë¬¸ì„œ ê²€ìƒ‰
)

# RAG í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿
template = """ë‹¤ìŒ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì‚¬ìš©í•˜ì—¬ ì§ˆë¬¸ì— ë‹µë³€í•˜ì„¸ìš”.
ë‹µì„ ëª¨ë¥´ë©´ ëª¨ë¥¸ë‹¤ê³  ë‹µë³€í•˜ì„¸ìš”. ë‹µë³€ì€ ê°„ê²°í•˜ê²Œ 3-4 ë¬¸ì¥ìœ¼ë¡œ ì‘ì„±í•˜ì„¸ìš”.

ì»¨í…ìŠ¤íŠ¸: {context}

ì§ˆë¬¸: {question}

ë‹µë³€:"""

prompt = ChatPromptTemplate.from_template(template)

# ë¬¸ì„œ í¬ë§·íŒ… í•¨ìˆ˜
def format_docs(docs):
    return "\n\n".join(doc.page_content for doc in docs)

# LCEL Chain êµ¬ì„±
rag_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }
    | prompt
    | llm
    | StrOutputParser()
)

print("âœ… RAG Chain êµ¬ì„± ì™„ë£Œ")

# ==========================================
# 7. ì§ˆì˜ì‘ë‹µ ì‹¤í–‰
# ==========================================

# ì‚¬ìš©ì ì§ˆë¬¸
question = "LGì´ë…¸í…ì˜ ì£¼ìš” ì œí’ˆì€ ë¬´ì—‡ì¸ê°€ìš”?"

print(f"\nâ“ ì§ˆë¬¸: {question}")
print("="*60)

# RAG ì‹¤í–‰
answer = rag_chain.invoke(question)

# ê²°ê³¼ ì¶œë ¥
print(f"\nğŸ’¡ ë‹µë³€:\n{answer}")

# ì°¸ì¡° ë¬¸ì„œ í™•ì¸
print("\nğŸ“š ì°¸ì¡° ë¬¸ì„œ:")
docs = retriever.invoke(question)
for i, doc in enumerate(docs, 1):
    print(f"  {i}. {doc.page_content} (ì¶œì²˜: {doc.metadata.get('source', 'N/A')})")

# ==========================================
# 8. ì¶”ê°€ ì§ˆë¬¸ ì˜ˆì œ
# ==========================================

def ask_question(question: str):
    """RAG ì‹œìŠ¤í…œì— ì§ˆë¬¸í•˜ëŠ” í—¬í¼ í•¨ìˆ˜"""
    print(f"\n{'='*60}")
    print(f"â“ ì§ˆë¬¸: {question}")
    print('='*60)

    # RAG ì‹¤í–‰
    answer = rag_chain.invoke(question)

    print(f"\nğŸ’¡ ë‹µë³€:\n{answer}")

    # ì°¸ì¡° ë¬¸ì„œ ì¶œë ¥
    print("\nğŸ“š ì°¸ì¡° ë¬¸ì„œ:")
    docs = retriever.invoke(question)
    for i, doc in enumerate(docs, 1):
        print(f"  {i}. {doc.page_content}")

    return {"answer": answer, "source_documents": docs}

# ì—¬ëŸ¬ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸
if __name__ == "__main__":
    questions = [
        "LGì´ë…¸í…ì€ ì–¸ì œ ì„¤ë¦½ë˜ì—ˆë‚˜ìš”?",
        "LGì´ë…¸í…ì˜ ë³¸ì‚¬ëŠ” ì–´ë””ì— ìˆë‚˜ìš”?",
        "LGì´ë…¸í…ì˜ ì£¼ìš” ê³ ê°ì€ ëˆ„êµ¬ì¸ê°€ìš”?",
    ]

    for q in questions:
        ask_question(q)
