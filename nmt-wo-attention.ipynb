{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading train dataset & tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting konlpy\n",
      "  Downloading konlpy-0.6.0-py2.py3-none-any.whl.metadata (1.9 kB)\n",
      "Collecting JPype1>=0.7.0 (from konlpy)\n",
      "  Downloading jpype1-1.6.0-cp310-cp310-macosx_10_9_universal2.whl.metadata (5.0 kB)\n",
      "Requirement already satisfied: lxml>=4.1.0 in /Users/yhkim/miniconda3/envs/pytorch/lib/python3.10/site-packages (from konlpy) (4.9.4)\n",
      "Requirement already satisfied: numpy>=1.6 in /Users/yhkim/miniconda3/envs/pytorch/lib/python3.10/site-packages (from konlpy) (1.25.1)\n",
      "Requirement already satisfied: packaging in /Users/yhkim/miniconda3/envs/pytorch/lib/python3.10/site-packages (from JPype1>=0.7.0->konlpy) (23.1)\n",
      "Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.4/19.4 MB\u001b[0m \u001b[31m80.8 kB/s\u001b[0m  \u001b[33m0:03:10\u001b[0mm0:00:01\u001b[0m00:08\u001b[0mm\n",
      "\u001b[?25hDownloading jpype1-1.6.0-cp310-cp310-macosx_10_9_universal2.whl (583 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m583.4/583.4 kB\u001b[0m \u001b[31m94.7 kB/s\u001b[0m  \u001b[33m0:00:07\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: JPype1, konlpy\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [konlpy]2m1/2\u001b[0m [konlpy]\n",
      "\u001b[1A\u001b[2KSuccessfully installed JPype1-1.6.0 konlpy-0.6.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: A restricted method in java.lang.System has been called\n",
      "WARNING: java.lang.System::load has been called by org.jpype.JPypeContext in an unnamed module (file:/Users/yhkim/miniconda3/envs/pytorch/lib/python3.10/site-packages/org.jpype.jar)\n",
      "WARNING: Use --enable-native-access=ALL-UNNAMED to avoid a warning for callers in this module\n",
      "WARNING: Restricted methods will be blocked in a future release unless native access is enabled\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from konlpy.tag import Hannanum \n",
    "morph = Hannanum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading kor.txt...\n",
      "kor.txt downloaded.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Download data files if not exist\n",
    "data_urls = {\n",
    "    'kor.txt': 'https://github.com/nongaussian/class-2026-lginnotek-llm/raw/refs/heads/main/nmt/kor.txt'\n",
    "}\n",
    "\n",
    "for filename, url in data_urls.items():\n",
    "    if not os.path.exists(filename):\n",
    "        print(f'Downloading {filename}...')\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(f'{filename} downloaded.')\n",
    "    else:\n",
    "        print(f'{filename} already exists.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kor.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "targ, inp = zip(*[line.split('\\t') for line in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'의심의 여지 없이 세상에는 어떤 남자이든 정확히 딱 알맞는 여자와 결혼하거나 그 반대의 상황이 존재하지. 그런데 인간이 수백 명의 사람만 알고 지내는 사이가 될 기회를 갖는다고 생각해 보면, 또 그 수백 명 중 열여 명 쯤 이하만 잘 알 수 있고, 그리고 나서 그 열여 명 중에 한두 명만 친구가 될 수 있다면, 그리고 또 만일 우리가 이 세상에 살고 있는 수백만 명의 사람들만 기억하고 있다면, 딱 맞는 남자는 지구가 생겨난 이래로 딱 맞는 여자를 단 한번도 만난 적이 없을 수도 있을 거라는 사실을 쉽게 눈치챌 수 있을 거야.\\n'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['의심', '의', '여', '이', '지', '없이', '세상', '에는', '어떤', '남자', '이', '든', '정확히', '딱', '알맞', '는', '여자', '와', '결혼', '하', '거나', '그', '반대', '의', '상황', '이', '존재', '하', '지', '.', '그런데', '인간', '이', '수백', '명', '의', '사람', '만', '알', '고', '지내', '는', '사이', '가', '되', 'ㄹ', '기회', '를', '갖', '는다', '고', '생각', '하', '어', '보', '면', ',', '또', '그', '수백', '명', '중', '열', '여', '명', '쯤', '이하', '만', '잘', '알', 'ㄹ', '수', '있', '고', ',', '그리고', '나', '서', '그', '열', '여', '명', '중', '에', '한두', '명', '만', '친구', '가', '되', 'ㄹ', '수', '있', '다면', ',', '그리고', '또', '만', '이', 'ㄹ', '우리', '가', '이', '세상', '에', '살', '고', '있', '는', '수백만', '명', '의', '사람들', '만', '기억', '하고', '있', '다면', ',', '딱', '맞', '는', '남자', '는', '지구', '가', '생기', '어', '나', 'ㄴ', '이래', '로', '딱', '맞', '는', '여자', '를', '달', 'ㄴ', '한번', '도', '만나', 'ㄴ', '적', '이', '없', '을', '수', '도', '있', '을', '것', '이', '라는', '사실', '을', '쉽', '게', '눈치채', 'ㄹ', '수', '있', '을', '것', '이', '야', '.']\n"
     ]
    }
   ],
   "source": [
    "print(morph.morphs(inp[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ[-1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltk\n",
      "  Downloading nltk-3.9.2-py3-none-any.whl.metadata (3.2 kB)\n",
      "Requirement already satisfied: click in /Users/yhkim/miniconda3/envs/pytorch/lib/python3.10/site-packages (from nltk) (8.1.6)\n",
      "Requirement already satisfied: joblib in /Users/yhkim/miniconda3/envs/pytorch/lib/python3.10/site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Users/yhkim/miniconda3/envs/pytorch/lib/python3.10/site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in /Users/yhkim/miniconda3/envs/pytorch/lib/python3.10/site-packages (from nltk) (4.67.1)\n",
      "Downloading nltk-3.9.2-py3-none-any.whl (1.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.5/1.5 MB\u001b[0m \u001b[31m1.1 MB/s\u001b[0m  \u001b[33m0:00:01\u001b[0m eta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: nltk\n",
      "Successfully installed nltk-3.9.2\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to /Users/yhkim/nltk_data...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['doubtless', 'there', 'exists', 'in', 'this', 'world', 'precisely', 'the', 'right', 'woman', 'for', 'any', 'given', 'man', 'to', 'marry', 'and', 'vice', 'versa', ';', 'but', 'when', 'you', 'consider', 'that', 'a', 'human', 'being', 'has', 'the', 'opportunity', 'of', 'being', 'acquainted', 'with', 'only', 'a', 'few', 'hundred', 'people', ',', 'and', 'out', 'of', 'the', 'few', 'hundred', 'that', 'there', 'are', 'but', 'a', 'dozen', 'or', 'less', 'whom', 'he', 'knows', 'intimately', ',', 'and', 'out', 'of', 'the', 'dozen', ',', 'one', 'or', 'two', 'friends', 'at', 'most', ',', 'it', 'will', 'easily', 'be', 'seen', ',', 'when', 'we', 'remember', 'the', 'number', 'of', 'millions', 'who', 'inhabit', 'this', 'world', ',', 'that', 'probably', ',', 'since', 'the', 'earth', 'was', 'created', ',', 'the', 'right', 'man', 'has', 'never', 'yet', 'met', 'the', 'right', 'woman', '.']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt_tab')\n",
    "print(nltk.word_tokenize(targ[-1].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokens = [ morph.morphs(x) for x in inp ]\n",
    "y_tokens = [ nltk.word_tokenize(x.lower()) for x in targ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3729 3729\n"
     ]
    }
   ],
   "source": [
    "print(len(x_tokens), len(y_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding text to numeric sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_encoding(lines):\n",
    "    vocab, index = {}, 3  # start indexing from 3\n",
    "    vocab['<pad>'] = 0  # add a padding token\n",
    "    vocab['<bos>'] = 1  # begin of sentence\n",
    "    vocab['<eos>'] = 2  # end of sentence\n",
    "\n",
    "    maxlen = -1\n",
    "    for sentence in lines:\n",
    "        for token in sentence:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = index\n",
    "                index += 1\n",
    "        \n",
    "        if maxlen < len(sentence):\n",
    "            maxlen = len(sentence)\n",
    "    \n",
    "    arr = np.zeros((len(lines), maxlen+2), dtype='int32')\n",
    "    \n",
    "    for i, sentence in enumerate(lines):\n",
    "        for j, token in enumerate(sentence):\n",
    "            arr[i, j+1] = vocab[token]\n",
    "        arr[i, 0] = vocab['<bos>']\n",
    "        arr[i, len(sentence)+1] = vocab['<eos>']\n",
    "    \n",
    "    return arr, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_vocab = text_encoding(x_tokens)\n",
    "y_train, y_vocab = text_encoding(y_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_x_vocab = {index: token for token, index in x_vocab.items()}\n",
    "inverse_y_vocab = {index: token for token, index in y_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_decoding(line, invvocab):\n",
    "    return [ invvocab[x] for x in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', '의심', '의', '여', '이', '지', '없이', '세상', '에는', '어떤', '남자', '이', '든', '정확히', '딱', '알맞', '는', '여자', '와', '결혼', '하', '거나', '그', '반대', '의', '상황', '이', '존재', '하', '지', '.', '그런데', '인간', '이', '수백', '명', '의', '사람', '만', '알', '고', '지내', '는', '사이', '가', '되', 'ㄹ', '기회', '를', '갖', '는다', '고', '생각', '하', '어', '보', '면', ',', '또', '그', '수백', '명', '중', '열', '여', '명', '쯤', '이하', '만', '잘', '알', 'ㄹ', '수', '있', '고', ',', '그리고', '나', '서', '그', '열', '여', '명', '중', '에', '한두', '명', '만', '친구', '가', '되', 'ㄹ', '수', '있', '다면', ',', '그리고', '또', '만', '이', 'ㄹ', '우리', '가', '이', '세상', '에', '살', '고', '있', '는', '수백만', '명', '의', '사람들', '만', '기억', '하고', '있', '다면', ',', '딱', '맞', '는', '남자', '는', '지구', '가', '생기', '어', '나', 'ㄴ', '이래', '로', '딱', '맞', '는', '여자', '를', '달', 'ㄴ', '한번', '도', '만나', 'ㄴ', '적', '이', '없', '을', '수', '도', '있', '을', '것', '이', '라는', '사실', '을', '쉽', '게', '눈치채', 'ㄹ', '수', '있', '을', '것', '이', '야', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(text_decoding(x_train[-1], inverse_x_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<bos>', 'doubtless', 'there', 'exists', 'in', 'this', 'world', 'precisely', 'the', 'right', 'woman', 'for', 'any', 'given', 'man', 'to', 'marry', 'and', 'vice', 'versa', ';', 'but', 'when', 'you', 'consider', 'that', 'a', 'human', 'being', 'has', 'the', 'opportunity', 'of', 'being', 'acquainted', 'with', 'only', 'a', 'few', 'hundred', 'people', ',', 'and', 'out', 'of', 'the', 'few', 'hundred', 'that', 'there', 'are', 'but', 'a', 'dozen', 'or', 'less', 'whom', 'he', 'knows', 'intimately', ',', 'and', 'out', 'of', 'the', 'dozen', ',', 'one', 'or', 'two', 'friends', 'at', 'most', ',', 'it', 'will', 'easily', 'be', 'seen', ',', 'when', 'we', 'remember', 'the', 'number', 'of', 'millions', 'who', 'inhabit', 'this', 'world', ',', 'that', 'probably', ',', 'since', 'the', 'earth', 'was', 'created', ',', 'the', 'right', 'man', 'has', 'never', 'yet', 'met', 'the', 'right', 'woman', '.', '<eos>']\n"
     ]
    }
   ],
   "source": [
    "print(text_decoding(y_train[-1], inverse_y_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2896 2527\n"
     ]
    }
   ],
   "source": [
    "print(len(x_vocab), len(y_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3729, 169)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('kor-eng', x_train=x_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"kor-eng-krvocab.json\", \"w\") as f:\n",
    "    json.dump(x_vocab, f)\n",
    "with open(\"kor-eng-envocab.json\", \"w\") as f:\n",
    "    json.dump(y_vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "npzfile = np.load('kor-eng.npz')\n",
    "x_train = npzfile['x_train']\n",
    "y_train = npzfile['y_train']\n",
    "\n",
    "with open(\"kor-eng-krvocab.json\", \"rb\") as f:\n",
    "    x_vocab = json.load(f)\n",
    "with open(\"kor-eng-envocab.json\", \"rb\") as f:\n",
    "    y_vocab = json.load(f)\n",
    "    \n",
    "inverse_x_vocab = {index: token for token, index in x_vocab.items()}\n",
    "inverse_y_vocab = {index: token for token, index in y_vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(x_train)\n",
    "BATCH_SIZE = 16\n",
    "embedding_dim = 1024\n",
    "latent_dim = 1024\n",
    "x_vocab_size = len(x_vocab)\n",
    "y_vocab_size = len(y_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = torch.LongTensor(x_data)\n",
    "        self.y_data = torch.LongTensor(y_data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "\n",
    "dataset = TranslationDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1,   60,   48,   25,    3,  878,   21,    8,   68,  315,  134,   72,\n",
      "           21,   27,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [   1,   80,   38,    8,    5,    2,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [   1,  779,    8,  247,   58,   48,   52,   25,    3,   72,   21, 1238,\n",
      "          387,   48,   27,    5,    2,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [   1,   60,  110,  320,  136, 1135,   21,    8,    5,    2,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0],\n",
      "        [   1,  940,   46, 1949,  576, 1002,  463,  110,  626,   48,  201,  387,\n",
      "          803,    4,    5,    2,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0]])\n",
      "\n",
      "tensor([[   1,   56,  560,   17,  249,  844,    4,    2,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [   1,  225,  460,    4,    2,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [   1, 1113,   60,  222,   90,  109,   17,  813,    4,    2,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [   1,   56,  204,  574, 1025,    4,    2,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0],\n",
      "        [   1,   17,  584,  222,  836,  204,  395, 1232,  398,  810,   48,  395,\n",
      "         1261,    4,    2,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "for example_input_batch, example_target_batch in dataloader:\n",
    "    print(example_input_batch[:5])\n",
    "    print()\n",
    "    print(example_target_batch[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact batch (remove trailing zeros)\n",
    "def compact_batch(batch):\n",
    "    max_len = (batch != 0).sum(dim=1).max().item()\n",
    "    return batch[:, :max_len]\n",
    "\n",
    "example_input_batch = compact_batch(example_input_batch)\n",
    "example_target_batch = compact_batch(example_target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   1,   60,  110,  759,    3,  312,   54,  135,   57,  117,  850,    3,\n",
      "          515,  274,  134,  103,   48,  134,  135,    8,    5,    2,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   1,   60,   48,  779,   52,   21,  112,  325, 1156,  260,    3, 2570,\n",
      "           21,   42,  130,  135,   61,  387,   48,  153,  134,  391,   21,  112,\n",
      "          325,   23,    5,    2],\n",
      "        [   1,   25,   46, 1167,  204, 2041,   61,  122,   46,  387,   61,  343,\n",
      "            4,   21,   76,    5,    2,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   1,   48,   48,  335, 2399,    3,  121,  117,  104,   97,    5,    2,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0],\n",
      "        [   1,  256,   48,  617, 1517,  626,   65,  542,   21,    8,    5,    2,\n",
      "            0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
      "            0,    0,    0,    0]])\n"
     ]
    }
   ],
   "source": [
    "print(example_input_batch[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([16, 169])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = (example_input_batch != 0).sum(dim=1).cpu()\n",
    "example_input_batch_packed = nn.utils.rnn.pack_padded_sequence(\n",
    "    example_input_batch, max_len, batch_first=True, enforce_sorted=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PackedSequence(data=tensor([   1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1,\n",
       "           1,    1,    1,    1,  779,  940,   60,  386,   97, 1834,   58,  260,\n",
       "          25,  587,   10,   60,   60, 1249,   80,  458,    8,   46,   48,  201,\n",
       "        1254,   46,   38,  750,  201,   46,    3,  110,  110,   40,   38,   13,\n",
       "         247, 1949,   25,  940, 1372,  535,   48,   65,  934, 1480, 1330,  320,\n",
       "         840, 1250,    8,    8,   58,  576,    3,  184,  769,  359,  759,  476,\n",
       "         935,  136,   61,  136,  841,   40,    5,    5,   48, 1002,  878,  571,\n",
       "         626,  712,  184,   21,   60,   58, 1219, 1135,  263,   11,    2,    2,\n",
       "          52,  463,   21,   42,  136, 1835,  911,    8,   61,  184,  265,   21,\n",
       "           5,    2,   25,  110,    8,  130,   72,  139,   52,   68,  936, 1481,\n",
       "         135,    8,    2,    3,  626,   68,  135,   21, 1836,   21,    8,  263,\n",
       "          94,    8,    5,   72,   48,  315,    8,    8,    8,   94,    5,    5,\n",
       "           5,   11,    2,   21,  201,  134,    5,   11,    5,    5,    2,    2,\n",
       "           2,    2, 1238,  387,   72,    2,    2,    2,    2,  387,  803,   21,\n",
       "          48,    4,   27,   27,    5,    2,    5,    2,    2]), batch_sizes=tensor([16, 16, 16, 16, 16, 16, 14, 13, 12, 12, 11,  7,  3,  3,  3,  2,  1]), sorted_indices=tensor([ 2,  4,  0, 15, 14,  5,  7,  6, 13, 10, 12,  3, 11,  9,  1,  8]), unsorted_indices=tensor([ 2, 14,  0, 11,  1,  5,  7,  6, 15, 13,  9, 12, 10,  8,  4,  3]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input_batch_packed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=3):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        # Create mask for padding\n",
    "        lengths = (x != 0).sum(dim=1).cpu()\n",
    "        \n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embedding_dim)\n",
    "        \n",
    "        # Pack sequence for efficient computation\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        packed_out, hidden = self.gru(packed)\n",
    "        \n",
    "        # Unpack sequence\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        \n",
    "        # hidden: (num_layers, batch, hidden_dim)\n",
    "        # Return each layer's hidden state separately for compatibility\n",
    "        s1 = hidden[0]  # (batch, hidden_dim)\n",
    "        s2 = hidden[1]  # (batch, hidden_dim)\n",
    "        s3 = hidden[2]  # (batch, hidden_dim)\n",
    "        \n",
    "        return output, s1, s2, s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder(\n",
      "  (embedding): Embedding(2896, 1024, padding_idx=0)\n",
      "  (gru): GRU(1024, 1024, num_layers=3, batch_first=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "encoder = Encoder(x_vocab_size, embedding_dim, latent_dim).to(device)\n",
    "print(encoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 17, 1024]) torch.Size([16, 1024])\n"
     ]
    }
   ],
   "source": [
    "last_output, last_state1, last_state2, last_state3 = \\\n",
    "    encoder(example_input_batch.to(device))\n",
    "print(last_output.shape, last_state1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers=3):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, s1, s2, s3):\n",
    "        # x: (batch, seq_len)\n",
    "        # s1, s2, s3: (batch, hidden_dim)\n",
    "        \n",
    "        # Get lengths for packing\n",
    "        lengths = (x != 0).sum(dim=1).cpu()\n",
    "        lengths = lengths.clamp(min=1)  # Ensure minimum length of 1\n",
    "        \n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embedding_dim)\n",
    "        \n",
    "        # Pack sequence\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Stack hidden states from all layers: (num_layers, batch, hidden)\n",
    "        hidden = torch.stack([s1, s2, s3], dim=0)\n",
    "        \n",
    "        packed_out, out_hidden = self.gru(packed, hidden)\n",
    "        \n",
    "        # Unpack sequence\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        \n",
    "        logits = self.fc(output)  # (batch, seq_len, vocab_size)\n",
    "        \n",
    "        # Return each layer's hidden state separately for compatibility\n",
    "        out_s1 = out_hidden[0]  # (batch, hidden_dim)\n",
    "        out_s2 = out_hidden[1]  # (batch, hidden_dim)\n",
    "        out_s3 = out_hidden[2]  # (batch, hidden_dim)\n",
    "        \n",
    "        return logits, out_s1, out_s2, out_s3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Decoder(\n",
      "  (embedding): Embedding(2527, 1024, padding_idx=0)\n",
      "  (gru): GRU(1024, 1024, num_layers=3, batch_first=True)\n",
      "  (fc): Linear(in_features=1024, out_features=2527, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "decoder = Decoder(y_vocab_size, embedding_dim, latent_dim).to(device)\n",
    "logits, s1, s2, s3 = decoder(example_target_batch.to(device), \\\n",
    "                             last_state1, last_state2, last_state3)\n",
    "print(decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 15, 2527]) torch.Size([16, 1024]) torch.Size([16, 1024]) torch.Size([16, 1024])\n"
     ]
    }
   ],
   "source": [
    "print(logits.shape, s1.shape, s2.shape, s3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss(y_true, y_pred):\n",
    "    # y_true: (batch, seq_len)\n",
    "    # y_pred: (batch, seq_len, vocab_size)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    # Reshape for CrossEntropyLoss: (batch * seq_len, vocab_size)\n",
    "    batch_size, seq_len, vocab_size = y_pred.shape\n",
    "    y_pred_flat = y_pred.reshape(-1, vocab_size)\n",
    "    y_true_flat = y_true.reshape(-1)\n",
    "    \n",
    "    loss = loss_fn(y_pred_flat, y_true_flat)\n",
    "    loss = loss.reshape(batch_size, seq_len)\n",
    "    \n",
    "    # Mask padding tokens\n",
    "    mask = (y_true != 0).float()\n",
    "    loss = loss * mask\n",
    "    \n",
    "    return loss.sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss(example_target_batch[:, 1:].to(device), logits[:, :-1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_batch, y_batch, training=True):\n",
    "    encoder.train(training)\n",
    "    decoder.train(training)\n",
    "    \n",
    "    _, s1, s2, s3 = encoder(x_batch)\n",
    "    logits, _, _, _ = decoder(y_batch, s1, s2, s3)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_batch, y_batch):\n",
    "    # Compact batch tensors\n",
    "    x_batch = compact_batch(x_batch).to(device)\n",
    "    y_batch = compact_batch(y_batch).to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Encoder & decoder\n",
    "    logits = predict(x_batch, y_batch, training=True)\n",
    "    \n",
    "    # Loss: compare y_batch[:, 1:] with logits[:, :-1, :]\n",
    "    loss = batch_loss(y_batch[:, 1:], logits[:, :-1, :])\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    start = time.time()\n",
    "    \n",
    "    loss_sum = 0\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        loss_sum += loss\n",
    "    \n",
    "    print('Time for epoch {} is {:.2f} sec: training loss = {:.6f}'.format(\n",
    "        epoch + 1, time.time() - start, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "torch.save(encoder.state_dict(), 'nmt-wo-attention.encoder.pt')\n",
    "torch.save(decoder.state_dict(), 'nmt-wo-attention.decoder.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(src, max_steps=100):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Tokenization\n",
    "    src_tokens = np.array([x_vocab['<bos>']] + [x_vocab[x] for x in morph.morphs(src)] + [x_vocab['<eos>']])\n",
    "    \n",
    "    print([inverse_x_vocab[x] for x in src_tokens])\n",
    "    \n",
    "    # Add the batch axis\n",
    "    x_test = torch.LongTensor(src_tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Compute encoder and get hidden states\n",
    "        _, s1, s2, s3 = encoder(x_test)\n",
    "        \n",
    "        # y_test: add the batch axis\n",
    "        y_test = torch.LongTensor([[y_vocab['<bos>']]]).to(device)\n",
    "        output_seq = []\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            logits, s1, s2, s3 = decoder(y_test, s1, s2, s3)\n",
    "            \n",
    "            # Greedily use the token with the highest logit\n",
    "            y_test = logits.argmax(dim=2)\n",
    "            pred = y_test.squeeze(0).item()\n",
    "            \n",
    "            # If prediction is eos, output sequence is complete\n",
    "            if pred == y_vocab['<eos>']:\n",
    "                break\n",
    "            output_seq.append(pred)\n",
    "    \n",
    "    return ' '.join([inverse_y_vocab[x] for x in output_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('잘 안된다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_translate(src, max_steps=100, k=16):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Tokenization\n",
    "    src_tokens = np.array([x_vocab['<bos>']] + [x_vocab[x] for x in morph.morphs(src)] + [x_vocab['<eos>']])\n",
    "    print(morph.morphs(src))\n",
    "    \n",
    "    # Add the batch axis\n",
    "    x_test = torch.LongTensor(src_tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Compute encoder and get hidden states\n",
    "        _, s1, s2, s3 = encoder(x_test)\n",
    "        \n",
    "        # Init candidates: (score, last_token, s1, s2, s3, output_seq, eos)\n",
    "        last_token = torch.LongTensor([[y_vocab['<bos>']]]).to(device)\n",
    "        candidates = [(0., last_token, s1, s2, s3, [y_vocab['<bos>']], False)]\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            new_candidates = []\n",
    "            \n",
    "            for score, token, c_s1, c_s2, c_s3, output_seq, eos in candidates:\n",
    "                # If the candidate already ends\n",
    "                if eos:\n",
    "                    new_candidates.append((score, token, c_s1, c_s2, c_s3, output_seq, eos))\n",
    "                    continue\n",
    "                \n",
    "                # Compute the prob. of following tokens\n",
    "                logits, new_s1, new_s2, new_s3 = decoder(token, c_s1, c_s2, c_s3)\n",
    "                # shape of logits: (1, 1, vocab_size)\n",
    "                probs = torch.log_softmax(logits, dim=2)\n",
    "                \n",
    "                # Use the token with the top-k logits\n",
    "                values, indices = torch.topk(probs.squeeze(), k=k)\n",
    "                \n",
    "                for prob, idx in zip(values, indices):\n",
    "                    idx_val = idx.item()\n",
    "                    # If prediction is eos, output sequence is complete\n",
    "                    is_eos = (idx_val == y_vocab['<eos>'])\n",
    "                    \n",
    "                    new_token = torch.LongTensor([[idx_val]]).to(device)\n",
    "                    new_candidates.append(\n",
    "                        (score + prob.item(), new_token, new_s1, new_s2, new_s3,\n",
    "                         output_seq + [idx_val], is_eos)\n",
    "                    )\n",
    "            \n",
    "            candidates = sorted(new_candidates, key=lambda t: -t[0])[:k]\n",
    "    \n",
    "    return [(candidates[i][0], ' '.join([inverse_y_vocab[x] for x in candidates[i][5]])) for i in range(k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_translate('잘 안된다.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
