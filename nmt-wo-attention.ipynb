{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading train dataset & tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install konlpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from konlpy.tag import Hannanum \n",
    "morph = Hannanum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('kor.txt', 'r') as f:\n",
    "    lines = f.readlines()\n",
    "    \n",
    "targ, inp = zip(*[line.split('\\t') for line in lines])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(morph.morphs(inp[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "targ[-1].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "print(nltk.word_tokenize(targ[-1].lower()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tokens = [ morph.morphs(x) for x in inp ]\n",
    "y_tokens = [ nltk.word_tokenize(x.lower()) for x in targ ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_tokens), len(y_tokens))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoding text to numeric sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_encoding(lines):\n",
    "    vocab, index = {}, 3  # start indexing from 3\n",
    "    vocab['<pad>'] = 0  # add a padding token\n",
    "    vocab['<bos>'] = 1  # begin of sentence\n",
    "    vocab['<eos>'] = 2  # end of sentence\n",
    "    preprocessed_tokens = []\n",
    "\n",
    "    maxlen = -1\n",
    "    for sentence in lines:\n",
    "        for token in sentence:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = index\n",
    "                index += 1\n",
    "        \n",
    "        if maxlen < len(sentence):\n",
    "            maxlen = len(sentence)\n",
    "    \n",
    "    arr = np.zeros((len(lines), maxlen+2), dtype='int32')\n",
    "    \n",
    "    for i, sentence in enumerate(lines):\n",
    "        for j, token in enumerate(sentence):\n",
    "            arr[i, j+1] = vocab[token]\n",
    "        arr[i, 0] = vocab['<bos>']\n",
    "        arr[i, len(sentence)+1] = vocab['<eos>']\n",
    "    \n",
    "    return arr, vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_vocab = text_encoding(x_tokens)\n",
    "y_train, y_vocab = text_encoding(y_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inverse_x_vocab = {index: token for token, index in x_vocab.items()}\n",
    "inverse_y_vocab = {index: token for token, index in y_vocab.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_decoding(line, invvocab):\n",
    "    return [ invvocab[x] for x in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_decoding(x_train[-1], inverse_x_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_decoding(y_train[-1], inverse_y_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(x_vocab), len(y_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savez('kor-eng', x_train=x_train, y_train=y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open(\"kor-eng-krvocab.json\", \"w\") as f:\n",
    "    json.dump(x_vocab, f)\n",
    "with open(\"kor-eng-envocab.json\", \"w\") as f:\n",
    "    json.dump(y_vocab, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load preprocessed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "npzfile = np.load('kor-eng.npz')\n",
    "x_train = npzfile['x_train']\n",
    "y_train = npzfile['y_train']\n",
    "\n",
    "with open(\"kor-eng-krvocab.json\", \"rb\") as f:\n",
    "    x_vocab = json.load(f)\n",
    "with open(\"kor-eng-envocab.json\", \"rb\") as f:\n",
    "    y_vocab = json.load(f)\n",
    "    \n",
    "inverse_x_vocab = {index: token for token, index in x_vocab.items()}\n",
    "inverse_y_vocab = {index: token for token, index in y_vocab.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = len(x_train)\n",
    "BATCH_SIZE = 16\n",
    "embedding_dim = 1024\n",
    "latent_dim = 1024\n",
    "x_vocab_size = len(x_vocab)\n",
    "y_vocab_size = len(y_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch Dataset\n",
    "class TranslationDataset(Dataset):\n",
    "    def __init__(self, x_data, y_data):\n",
    "        self.x_data = torch.LongTensor(x_data)\n",
    "        self.y_data = torch.LongTensor(y_data)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.x_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.x_data[idx], self.y_data[idx]\n",
    "\n",
    "dataset = TranslationDataset(x_train, y_train)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for example_input_batch, example_target_batch in dataloader:\n",
    "    print(example_input_batch[:5])\n",
    "    print()\n",
    "    print(example_target_batch[:5])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact batch (remove trailing zeros)\n",
    "def compact_batch(batch):\n",
    "    max_len = (batch != 0).sum(dim=1).max().item()\n",
    "    return batch[:, :max_len]\n",
    "\n",
    "example_input_batch = compact_batch(example_input_batch)\n",
    "example_target_batch = compact_batch(example_target_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru1 = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.gru2 = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.gru3 = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        # Create mask for padding\n",
    "        lengths = (x != 0).sum(dim=1).cpu()\n",
    "        \n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embedding_dim)\n",
    "        \n",
    "        # Pack sequence for efficient computation\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        packed_out, s1 = self.gru1(packed)\n",
    "        packed_out, s2 = self.gru2(packed_out)\n",
    "        packed_out, s3 = self.gru3(packed_out)\n",
    "        \n",
    "        # Unpack sequence\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        \n",
    "        # s1, s2, s3: (1, batch, hidden_dim) -> (batch, hidden_dim)\n",
    "        return output, s1.squeeze(0), s2.squeeze(0), s3.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(x_vocab_size, embedding_dim, latent_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_output, last_state1, last_state2, last_state3 = encoder(example_input_batch.to(device))\n",
    "print(last_output.shape, last_state1.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru1 = nn.GRU(embedding_dim, hidden_dim, batch_first=True)\n",
    "        self.gru2 = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.gru3 = nn.GRU(hidden_dim, hidden_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "        \n",
    "    def forward(self, x, s1, s2, s3):\n",
    "        # x: (batch, seq_len)\n",
    "        # s1, s2, s3: (batch, hidden_dim)\n",
    "        \n",
    "        # Get lengths for packing\n",
    "        lengths = (x != 0).sum(dim=1).cpu()\n",
    "        lengths = lengths.clamp(min=1)  # Ensure minimum length of 1\n",
    "        \n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embedding_dim)\n",
    "        \n",
    "        # Pack sequence\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(\n",
    "            embedded, lengths, batch_first=True, enforce_sorted=False\n",
    "        )\n",
    "        \n",
    "        # Add batch dimension for hidden states: (batch, hidden) -> (1, batch, hidden)\n",
    "        packed_out, out_s1 = self.gru1(packed, s1.unsqueeze(0))\n",
    "        packed_out, out_s2 = self.gru2(packed_out, s2.unsqueeze(0))\n",
    "        packed_out, out_s3 = self.gru3(packed_out, s3.unsqueeze(0))\n",
    "        \n",
    "        # Unpack sequence\n",
    "        output, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=True)\n",
    "        \n",
    "        logits = self.fc(output)  # (batch, seq_len, vocab_size)\n",
    "        \n",
    "        return logits, out_s1.squeeze(0), out_s2.squeeze(0), out_s3.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(y_vocab_size, embedding_dim, latent_dim).to(device)\n",
    "logits, s1, s2, s3 = decoder(example_target_batch.to(device), last_state1, last_state2, last_state3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits.shape, s1.shape, s2.shape, s3.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss(y_true, y_pred):\n",
    "    # y_true: (batch, seq_len)\n",
    "    # y_pred: (batch, seq_len, vocab_size)\n",
    "    loss_fn = nn.CrossEntropyLoss(reduction='none')\n",
    "    \n",
    "    # Reshape for CrossEntropyLoss: (batch * seq_len, vocab_size)\n",
    "    batch_size, seq_len, vocab_size = y_pred.shape\n",
    "    y_pred_flat = y_pred.reshape(-1, vocab_size)\n",
    "    y_true_flat = y_true.reshape(-1)\n",
    "    \n",
    "    loss = loss_fn(y_pred_flat, y_true_flat)\n",
    "    loss = loss.reshape(batch_size, seq_len)\n",
    "    \n",
    "    # Mask padding tokens\n",
    "    mask = (y_true != 0).float()\n",
    "    loss = loss * mask\n",
    "    \n",
    "    return loss.sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss(example_target_batch[:, 1:].to(device), logits[:, :-1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(list(encoder.parameters()) + list(decoder.parameters()), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_batch, y_batch, training=True):\n",
    "    encoder.train(training)\n",
    "    decoder.train(training)\n",
    "    \n",
    "    _, s1, s2, s3 = encoder(x_batch)\n",
    "    logits, _, _, _ = decoder(y_batch, s1, s2, s3)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_batch, y_batch):\n",
    "    # Compact batch tensors\n",
    "    x_batch = compact_batch(x_batch).to(device)\n",
    "    y_batch = compact_batch(y_batch).to(device)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Encoder & decoder\n",
    "    logits = predict(x_batch, y_batch, training=True)\n",
    "    \n",
    "    # Loss: compare y_batch[:, 1:] with logits[:, :-1, :]\n",
    "    loss = batch_loss(y_batch[:, 1:], logits[:, :-1, :])\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    start = time.time()\n",
    "    \n",
    "    loss_sum = 0\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        loss_sum += loss\n",
    "    \n",
    "    print('Time for epoch {} is {:.2f} sec: training loss = {:.6f}'.format(\n",
    "        epoch + 1, time.time() - start, loss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model weights\n",
    "torch.save(encoder.state_dict(), 'nmt-wo-attention.encoder.pt')\n",
    "torch.save(decoder.state_dict(), 'nmt-wo-attention.decoder.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(src, max_steps=100):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Tokenization\n",
    "    src_tokens = np.array([x_vocab['<bos>']] + [x_vocab[x] for x in morph.morphs(src)] + [x_vocab['<eos>']])\n",
    "    \n",
    "    print([inverse_x_vocab[x] for x in src_tokens])\n",
    "    \n",
    "    # Add the batch axis\n",
    "    x_test = torch.LongTensor(src_tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Compute encoder and get hidden states\n",
    "        _, s1, s2, s3 = encoder(x_test)\n",
    "        \n",
    "        # y_test: add the batch axis\n",
    "        y_test = torch.LongTensor([[y_vocab['<bos>']]]).to(device)\n",
    "        output_seq = []\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            logits, s1, s2, s3 = decoder(y_test, s1, s2, s3)\n",
    "            \n",
    "            # Greedily use the token with the highest logit\n",
    "            y_test = logits.argmax(dim=2)\n",
    "            pred = y_test.squeeze(0).item()\n",
    "            \n",
    "            # If prediction is eos, output sequence is complete\n",
    "            if pred == y_vocab['<eos>']:\n",
    "                break\n",
    "            output_seq.append(pred)\n",
    "    \n",
    "    return ' '.join([inverse_y_vocab[x] for x in output_seq])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "translate('잘 안된다.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_translate(src, max_steps=100, k=16):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    # Tokenization\n",
    "    src_tokens = np.array([x_vocab['<bos>']] + [x_vocab[x] for x in morph.morphs(src)] + [x_vocab['<eos>']])\n",
    "    print(morph.morphs(src))\n",
    "    \n",
    "    # Add the batch axis\n",
    "    x_test = torch.LongTensor(src_tokens).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Compute encoder and get hidden states\n",
    "        _, s1, s2, s3 = encoder(x_test)\n",
    "        \n",
    "        # Init candidates: (score, last_token, s1, s2, s3, output_seq, eos)\n",
    "        last_token = torch.LongTensor([[y_vocab['<bos>']]]).to(device)\n",
    "        candidates = [(0., last_token, s1, s2, s3, [y_vocab['<bos>']], False)]\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            new_candidates = []\n",
    "            \n",
    "            for score, token, c_s1, c_s2, c_s3, output_seq, eos in candidates:\n",
    "                # If the candidate already ends\n",
    "                if eos:\n",
    "                    new_candidates.append((score, token, c_s1, c_s2, c_s3, output_seq, eos))\n",
    "                    continue\n",
    "                \n",
    "                # Compute the prob. of following tokens\n",
    "                logits, new_s1, new_s2, new_s3 = decoder(token, c_s1, c_s2, c_s3)\n",
    "                # shape of logits: (1, 1, vocab_size)\n",
    "                probs = torch.log_softmax(logits, dim=2)\n",
    "                \n",
    "                # Use the token with the top-k logits\n",
    "                values, indices = torch.topk(probs.squeeze(), k=k)\n",
    "                \n",
    "                for prob, idx in zip(values, indices):\n",
    "                    idx_val = idx.item()\n",
    "                    # If prediction is eos, output sequence is complete\n",
    "                    is_eos = (idx_val == y_vocab['<eos>'])\n",
    "                    \n",
    "                    new_token = torch.LongTensor([[idx_val]]).to(device)\n",
    "                    new_candidates.append(\n",
    "                        (score + prob.item(), new_token, new_s1, new_s2, new_s3,\n",
    "                         output_seq + [idx_val], is_eos)\n",
    "                    )\n",
    "            \n",
    "            candidates = sorted(new_candidates, key=lambda t: -t[0])[:k]\n",
    "    \n",
    "    return [(candidates[i][0], ' '.join([inverse_y_vocab[x] for x in candidates[i][5]])) for i in range(k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "beam_translate('잘 안된다.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
