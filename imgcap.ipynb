{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using device: {device}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading imgcap.npz...\n",
      "imgcap.npz downloaded.\n",
      "Downloading imgcap_output_vocab.json...\n",
      "imgcap_output_vocab.json downloaded.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import urllib.request\n",
    "import os\n",
    "\n",
    "# Download data files if not exist\n",
    "data_urls = {\n",
    "    'imgcap.npz': 'https://github.com/nongaussian/class-2026-lginnotek-llm/raw/refs/heads/main/imgcap/imgcap.npz',\n",
    "    'imgcap_output_vocab.json': 'https://raw.githubusercontent.com/nongaussian/class-2026-lginnotek-llm/refs/heads/main/imgcap/imgcap_output_vocab.json'\n",
    "}\n",
    "\n",
    "for filename, url in data_urls.items():\n",
    "    if not os.path.exists(filename):\n",
    "        print(f'Downloading {filename}...')\n",
    "        urllib.request.urlretrieve(url, filename)\n",
    "        print(f'{filename} downloaded.')\n",
    "    else:\n",
    "        print(f'{filename} already exists.')\n",
    "\n",
    "npzfile = np.load('imgcap.npz')\n",
    "input_imgs = npzfile['input_imgs']\n",
    "output_seqs = npzfile['output_seqs']\n",
    "\n",
    "with open(\"imgcap_output_vocab.json\", \"rb\") as f:\n",
    "    output_vocab = json.load(f)\n",
    "\n",
    "y_vocab_size = len(output_vocab)\n",
    "inverse_output_vocab = {index: token for token, index in output_vocab.items()}\n",
    "\n",
    "# PyTorch uses (N, C, H, W) format, so we expand and transpose\n",
    "input_imgs = np.expand_dims(input_imgs, axis=1)  # (N, 1, H, W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_imgs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_seqs.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check image and caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(input_imgs[3931, 0, :, :], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_seqs[3931]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_decoding(line, invvocab):\n",
    "    return [invvocab[x] for x in line]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(text_decoding(output_seqs[3931], inverse_output_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 16\n",
    "embedding_dim = 1024\n",
    "latent_dim = 100\n",
    "output_vocab_size = len(output_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageCaptionDataset(Dataset):\n",
    "    def __init__(self, images, captions):\n",
    "        self.images = torch.FloatTensor(images)\n",
    "        self.captions = torch.LongTensor(captions)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.images[idx], self.captions[idx]\n",
    "\n",
    "dataset = ImageCaptionDataset(input_imgs, output_seqs)\n",
    "dataloader = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get example batch\n",
    "for example_input_batch, example_target_batch in dataloader:\n",
    "    plt.imshow(example_input_batch[5, 0].numpy(), cmap='gray')\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compact batch tensors\n",
    "max_len = (example_target_batch != 0).sum(dim=1).max().item()\n",
    "example_target_batch = example_target_batch[:, :max_len]\n",
    "example_target_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, latent_dim=100):\n",
    "        super(Encoder, self).__init__()\n",
    "        # LeNet-like architecture\n",
    "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)  # padding='same'\n",
    "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1, padding=0)  # padding='valid'\n",
    "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.fc1 = nn.Linear(16 * 5 * 5, 256)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.fc3 = nn.Linear(128, latent_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = self.pool1(x)\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.pool2(x)\n",
    "        x = self.flatten(x)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Encoder(latent_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "example_input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "last_state = encoder(example_input_batch.to(device))\n",
    "print(last_state.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, latent_dim):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)\n",
    "        self.gru = nn.GRU(embedding_dim, latent_dim, batch_first=True)\n",
    "        self.fc = nn.Linear(latent_dim, vocab_size)\n",
    "    \n",
    "    def forward(self, target_seq, initial_state):\n",
    "        # target_seq: (batch_size, seq_len)\n",
    "        # initial_state: (batch_size, latent_dim)\n",
    "        \n",
    "        embedded = self.embedding(target_seq)  # (batch_size, seq_len, embedding_dim)\n",
    "        \n",
    "        # GRU expects hidden state of shape (num_layers, batch_size, latent_dim)\n",
    "        hidden = initial_state.unsqueeze(0)  # (1, batch_size, latent_dim)\n",
    "        \n",
    "        output, hidden = self.gru(embedded, hidden)\n",
    "        # output: (batch_size, seq_len, latent_dim)\n",
    "        # hidden: (1, batch_size, latent_dim)\n",
    "        \n",
    "        logits = self.fc(output)  # (batch_size, seq_len, vocab_size)\n",
    "        \n",
    "        return logits, hidden.squeeze(0)  # return hidden as (batch_size, latent_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder = Decoder(y_vocab_size, embedding_dim, latent_dim).to(device)\n",
    "logits, s1 = decoder(example_target_batch.to(device), last_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(logits.shape, s1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def batch_loss(y_true, y_pred):\n",
    "    # y_true: (batch_size, seq_len)\n",
    "    # y_pred: (batch_size, seq_len, vocab_size)\n",
    "    \n",
    "    # Compute cross entropy loss\n",
    "    loss = F.cross_entropy(y_pred.reshape(-1, y_pred.size(-1)), \n",
    "                           y_true.reshape(-1), \n",
    "                           reduction='none')\n",
    "    loss = loss.view(y_true.shape)\n",
    "    \n",
    "    # Apply mask for padding tokens\n",
    "    mask = (y_true != 0).float()\n",
    "    loss = loss * mask\n",
    "    \n",
    "    return loss.sum() / mask.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_loss(example_target_batch[:, 1:].to(device), logits[:, :-1, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Re-initialize models\n",
    "encoder = Encoder(latent_dim).to(device)\n",
    "decoder = Decoder(y_vocab_size, embedding_dim, latent_dim).to(device)\n",
    "\n",
    "# Combine parameters for optimizer\n",
    "params = list(encoder.parameters()) + list(decoder.parameters())\n",
    "optimizer = torch.optim.Adam(params, lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(x_batch, y_batch, training=True):\n",
    "    last_state = encoder(x_batch)\n",
    "    logits, _ = decoder(y_batch, last_state)\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_batch, y_batch):\n",
    "    # 1. compact batch tensors\n",
    "    max_len = (y_batch != 0).sum(dim=1).max().item()\n",
    "    y_batch = y_batch[:, :max_len]\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # 2. encoder & decoder\n",
    "    logits = predict(x_batch, y_batch)\n",
    "    \n",
    "    # 3. loss\n",
    "    loss = batch_loss(y_batch[:, 1:], logits[:, :-1, :])\n",
    "    \n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in range(50):\n",
    "    start = time.time()\n",
    "    \n",
    "    encoder.train()\n",
    "    decoder.train()\n",
    "    \n",
    "    loss_sum = 0\n",
    "    for x_batch, y_batch in dataloader:\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "        loss = train_step(x_batch, y_batch)\n",
    "        loss_sum += loss\n",
    "    \n",
    "    print('Time for epoch {} is {:.2f} sec: training loss = {:.4f}'.format(\n",
    "        epoch + 1, time.time() - start, loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_translate(x_test, max_steps=100, k=16):\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # compute encoder and get hidden status\n",
    "        s1 = encoder(x_test)\n",
    "        \n",
    "        # init candidates\n",
    "        bos_token = output_vocab['<bos>']\n",
    "        last_token = torch.tensor([[bos_token]], device=device)\n",
    "        candidates = [(0., last_token, s1, [bos_token], False)]\n",
    "        \n",
    "        for _ in range(max_steps):\n",
    "            new_candidates = []\n",
    "            \n",
    "            for score, token, hidden, output_seq, eos in candidates:\n",
    "                # if the candidate already ends,\n",
    "                if eos:\n",
    "                    new_candidates.append((score, token, hidden, output_seq, eos))\n",
    "                    continue\n",
    "                \n",
    "                # compute the prob. of following tokens\n",
    "                logits, hidden = decoder(token, hidden)\n",
    "                # shape of logits (1, 1, vocab_size)\n",
    "                probs = F.log_softmax(logits, dim=2)\n",
    "                \n",
    "                # use the token with the top-k logits as the input\n",
    "                # of the decoder at the next time step\n",
    "                values, indices = torch.topk(probs.squeeze(), k=k)\n",
    "                \n",
    "                for prob, idx in zip(values, indices):\n",
    "                    idx_int = idx.item()\n",
    "                    # if prediction is eos, output sequence is complete\n",
    "                    eos = (idx_int == output_vocab['<eos>'])\n",
    "                    \n",
    "                    last_token = torch.tensor([[idx_int]], device=device)\n",
    "                    new_candidates.append(\n",
    "                        (score + prob.item(), last_token, hidden,\n",
    "                         output_seq + [idx_int], eos))\n",
    "            \n",
    "            candidates = sorted(new_candidates, key=lambda t: -t[0])[:k]\n",
    "        \n",
    "        return [(candidates[i][0], ' '.join([inverse_output_vocab[x] for x in candidates[i][3]])) \n",
    "                for i in range(k)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a fresh batch for testing\n",
    "for example_input_batch, example_target_batch in dataloader:\n",
    "    example_input_batch = example_input_batch.to(device)\n",
    "    break\n",
    "\n",
    "res = beam_translate(example_input_batch[1:2])\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(example_input_batch[1, 0].cpu().numpy(), cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
