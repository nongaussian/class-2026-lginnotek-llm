# !pip install langchain langchain-community langchain-google-vertexai faiss-cpu requests

from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import os
import json
import glob
import requests

# API í‚¤ ì„¤ì •
os.environ["GOOGLE_API_KEY"] = "your-api-key"

# GitHubì—ì„œ precedent_sample ë””ë ‰í† ë¦¬ì˜ JSON íŒŒì¼ë“¤ ë‹¤ìš´ë¡œë“œ
GITHUB_API_URL = "https://api.github.com/repos/nongaussian/class-2026-lginnotek-llm/contents/precedent_sample"
RAW_BASE_URL = "https://raw.githubusercontent.com/nongaussian/class-2026-lginnotek-llm/main/precedent_sample"
LOCAL_DIR = "precedent_sample"

# ë¡œì»¬ ë””ë ‰í† ë¦¬ ìƒì„± (ì—†ìœ¼ë©´)
os.makedirs(LOCAL_DIR, exist_ok=True)
print(f"ğŸ“ '{LOCAL_DIR}' ë””ë ‰í† ë¦¬ ì¤€ë¹„ ì™„ë£Œ")

# GitHub APIë¡œ íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
print("ğŸ” GitHubì—ì„œ íŒŒì¼ ëª©ë¡ì„ ê°€ì ¸ì˜¤ëŠ” ì¤‘...")
response = requests.get(GITHUB_API_URL)
if response.status_code != 200:
    raise Exception(f"GitHub API ìš”ì²­ ì‹¤íŒ¨: {response.status_code}")

files = response.json()
json_files_info = [f for f in files if f["name"].endswith(".json")]
print(f"ğŸ“‚ GitHubì—ì„œ {len(json_files_info)}ê°œì˜ JSON íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

# ê° JSON íŒŒì¼ ë‹¤ìš´ë¡œë“œ
for file_info in json_files_info:
    file_name = file_info["name"]
    local_path = os.path.join(LOCAL_DIR, file_name)

    # íŒŒì¼ ë‹¤ìš´ë¡œë“œ
    raw_url = f"{RAW_BASE_URL}/{file_name}"
    file_response = requests.get(raw_url)

    if file_response.status_code == 200:
        with open(local_path, "w", encoding="utf-8") as f:
            f.write(file_response.text)
        print(f"  âœ“ {file_name} ë‹¤ìš´ë¡œë“œ ì™„ë£Œ")
    else:
        print(f"  âœ— {file_name} ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {file_response.status_code}")

# ë¡œì»¬ ë””ë ‰í† ë¦¬ì—ì„œ JSON íŒŒì¼ë“¤ ë¡œë“œ
json_files = glob.glob(f"{LOCAL_DIR}/*.json")
print(f"\nğŸ“‚ {len(json_files)}ê°œì˜ JSON íŒŒì¼ì„ ë¡œë“œí•©ë‹ˆë‹¤.")

docs = []
for file_path in json_files:
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # íŒë¡€ ì •ë³´ ì¶”ì¶œ
    info = data.get("info", {})
    case_no = info.get("caseNo", "")
    case_nm = info.get("caseNm", "")
    court_nm = info.get("courtNm", "")
    judmn_date = info.get("judmnAdjuDe", "")

    # ë³¸ë¬¸ ë‚´ìš© êµ¬ì„± (ì‚¬ì‹¤ê´€ê³„ + ë²•ì› íŒë‹¨ + ê²°ë¡ )
    facts = data.get("facts", {}).get("bsisFacts", [])
    dcss = data.get("dcss", {}).get("courtDcss", [])
    cnclsns = data.get("close", {}).get("cnclsns", [])

    content = f"[ì‚¬ê±´ë²ˆí˜¸: {case_no}] {case_nm}\n"
    content += f"ë²•ì›: {court_nm} | ì„ ê³ ì¼: {judmn_date}\n\n"
    content += "[ì‚¬ì‹¤ê´€ê³„]\n" + "\n".join(facts) + "\n\n"
    content += "[ë²•ì› íŒë‹¨]\n" + "\n".join(dcss) + "\n\n"
    content += "[ê²°ë¡ ]\n" + "\n".join(cnclsns)

    # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ Document ìƒì„±
    doc = Document(
        page_content=content,
        metadata={
            "case_no": case_no,
            "case_nm": case_nm,
            "court_nm": court_nm,
            "judmn_date": judmn_date,
            "source": file_path
        }
    )
    docs.append(doc)

print(f"ğŸ“„ {len(docs)}ê°œì˜ ë¬¸ì„œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

# 2. ì„ë² ë”© ëª¨ë¸ ì„¤ì • (Google Generative AI)
# text-embedding-004ëŠ” Googleì˜ ìµœì‹  ì„ë² ë”© ëª¨ë¸ì…ë‹ˆë‹¤.
embeddings = GoogleGenerativeAIEmbeddings(model="models/gemini-embedding-001")

print("â³ ë¬¸ì„œ ë²¡í„°í™” ì§„í–‰ ì¤‘...")

# 3. FAISS ì¸ë±ìŠ¤ ìƒì„± (ë©”ëª¨ë¦¬ ìƒì— êµ¬ì¶•)
db = FAISS.from_documents(docs, embeddings)

# 4. ë¡œì»¬ ë””ìŠ¤í¬ì— ì €ì¥ (í•µì‹¬!)
# ì‹¤í–‰ ê²½ë¡œì— 'precedent_index'ë¼ëŠ” í´ë”ê°€ ìƒì„±ë©ë‹ˆë‹¤.
db.save_local("precedent_index")

print("âœ… ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ! ('precedent_index' í´ë” í™•ì¸)")