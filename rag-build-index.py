# !pip install langchain langchain-community langchain-google-vertexai faiss-cpu

from langchain_core.documents import Document
from langchain_community.vectorstores import FAISS
from langchain_google_genai import GoogleGenerativeAIEmbeddings
import os
import json
import glob

# API í‚¤ ì„¤ì •
os.environ["GOOGLE_API_KEY"] = "AIzaSyDWeeAT3iJ1nUAk3UrX1LVeIMlVv2gpBV4"

# precedent_sample ë””ë ‰í† ë¦¬ì—ì„œ JSON íŒŒì¼ë“¤ ë¡œë“œ
json_files = glob.glob("precedent_sample/*.json")
print(f"ğŸ“‚ {len(json_files)}ê°œì˜ JSON íŒŒì¼ì„ ë°œê²¬í–ˆìŠµë‹ˆë‹¤.")

docs = []
for file_path in json_files:
    with open(file_path, "r", encoding="utf-8") as f:
        data = json.load(f)

    # íŒë¡€ ì •ë³´ ì¶”ì¶œ
    info = data.get("info", {})
    case_no = info.get("caseNo", "")
    case_nm = info.get("caseNm", "")
    court_nm = info.get("courtNm", "")
    judmn_date = info.get("judmnAdjuDe", "")

    # ë³¸ë¬¸ ë‚´ìš© êµ¬ì„± (ì‚¬ì‹¤ê´€ê³„ + ë²•ì› íŒë‹¨ + ê²°ë¡ )
    facts = data.get("facts", {}).get("bsisFacts", [])
    dcss = data.get("dcss", {}).get("courtDcss", [])
    cnclsns = data.get("close", {}).get("cnclsns", [])

    content = f"[ì‚¬ê±´ë²ˆí˜¸: {case_no}] {case_nm}\n"
    content += f"ë²•ì›: {court_nm} | ì„ ê³ ì¼: {judmn_date}\n\n"
    content += "[ì‚¬ì‹¤ê´€ê³„]\n" + "\n".join(facts) + "\n\n"
    content += "[ë²•ì› íŒë‹¨]\n" + "\n".join(dcss) + "\n\n"
    content += "[ê²°ë¡ ]\n" + "\n".join(cnclsns)

    exit

    # ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ Document ìƒì„±
    doc = Document(
        page_content=content,
        metadata={
            "case_no": case_no,
            "case_nm": case_nm,
            "court_nm": court_nm,
            "judmn_date": judmn_date,
            "source": file_path
        }
    )
    docs.append(doc)

print(f"ğŸ“„ {len(docs)}ê°œì˜ ë¬¸ì„œë¥¼ ìƒì„±í–ˆìŠµë‹ˆë‹¤.")

# 2. ì„ë² ë”© ëª¨ë¸ ì„¤ì • (Google Generative AI)
# text-embedding-004ëŠ” Googleì˜ ìµœì‹  ì„ë² ë”© ëª¨ë¸ì…ë‹ˆë‹¤.
embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")

print("â³ ë¬¸ì„œ ë²¡í„°í™” ì§„í–‰ ì¤‘...")

# 3. FAISS ì¸ë±ìŠ¤ ìƒì„± (ë©”ëª¨ë¦¬ ìƒì— êµ¬ì¶•)
db = FAISS.from_documents(docs, embeddings)

# 4. ë¡œì»¬ ë””ìŠ¤í¬ì— ì €ì¥ (í•µì‹¬!)
# ì‹¤í–‰ ê²½ë¡œì— 'precedent_index'ë¼ëŠ” í´ë”ê°€ ìƒì„±ë©ë‹ˆë‹¤.
db.save_local("precedent_index")

print("âœ… ì¸ë±ìŠ¤ ì €ì¥ ì™„ë£Œ! ('precedent_index' í´ë” í™•ì¸)")