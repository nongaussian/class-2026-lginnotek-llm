{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"1\"\n",
    "\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "npz = np.load('stockprice5.npz')\n",
    "x_train = npz['x_train']\n",
    "y_train = npz['y_train']\n",
    "x_test = npz['x_test']\n",
    "y_test = npz['y_test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(13760, 60, 5) (13760, 1) (4065, 60, 5) (4065, 1)\n"
     ]
    }
   ],
   "source": [
    "print(x_train.shape, y_train.shape, x_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test LSTM & TimeDistributed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 4])\n",
      "torch.Size([1, 32, 4])\n"
     ]
    }
   ],
   "source": [
    "# PyTorch LSTM test\n",
    "# Input shape: (batch, seq_len, input_size)\n",
    "inputs = torch.randn(32, 10, 8)\n",
    "lstm = nn.LSTM(input_size=8, hidden_size=4, batch_first=True)\n",
    "output, (h_n, c_n) = lstm(inputs)\n",
    "print(output.shape)  # (batch, seq_len, hidden_size) - 모든 타임스텝의 출력\n",
    "print(h_n.shape)     # (num_layers, batch, hidden_size) - 마지막 hidden state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch LSTM returns both sequence outputs and states by default\n",
    "lstm = nn.LSTM(input_size=8, hidden_size=4, batch_first=True)\n",
    "whole_seq_output, (final_state_h, final_state_c) = lstm(inputs)\n",
    "print(whole_seq_output.shape)  # (32, 10, 4) - 전체 시퀀스 출력\n",
    "print(final_state_h.shape)     # (1, 32, 4) - 마지막 hidden state\n",
    "print(final_state_c.shape)     # (1, 32, 4) - 마지막 cell state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch TimeDistributed equivalent\n",
    "# For Conv2D over time, we reshape (batch, time, C, H, W) -> (batch*time, C, H, W)\n",
    "inputs = torch.randn(4, 10, 3, 128, 128)  # (batch, time, channels, H, W)\n",
    "batch, time, c, h, w = inputs.shape\n",
    "\n",
    "conv_2d_layer = nn.Conv2d(3, 64, kernel_size=3)\n",
    "# Reshape, apply conv, then reshape back\n",
    "inputs_reshaped = inputs.view(batch * time, c, h, w)\n",
    "outputs = conv_2d_layer(inputs_reshaped)\n",
    "outputs = outputs.view(batch, time, 64, 126, 126)\n",
    "print(outputs.shape)  # (4, 10, 64, 126, 126)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PyTorch RepeatVector equivalent\n",
    "# Dense(32) -> RepeatVector(3) : (batch, 32) -> (batch, 3, 32)\n",
    "x = torch.randn(4, 32)\n",
    "dense = nn.Linear(32, 32)\n",
    "x = dense(x)\n",
    "# RepeatVector: unsqueeze and repeat\n",
    "x = x.unsqueeze(1).repeat(1, 3, 1)\n",
    "print(x.shape)  # (4, 3, 32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 5\n",
    "n_rnn_layers = 3\n",
    "x_dims = 8\n",
    "latent_dims = 8 # assumed to be the same as x_dims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, x_dims, n_rnn_layers, dropout=0.2):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # TimeDistributed Dense equivalent: Linear layer applied to each timestep\n",
    "        self.fc_input = nn.Linear(input_dim, x_dims)\n",
    "        \n",
    "        # Stacked LSTM layers\n",
    "        # PyTorch LSTM can have multiple layers built-in\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=x_dims,\n",
    "            hidden_size=x_dims,\n",
    "            num_layers=n_rnn_layers,\n",
    "            batch_first=True,\n",
    "            dropout=dropout if n_rnn_layers > 1 else 0  # dropout between LSTM layers\n",
    "        )\n",
    "        \n",
    "        # Output layer\n",
    "        self.fc_output = nn.Linear(x_dims, 1)\n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_dim)\n",
    "        \n",
    "        # Apply linear layer to each timestep (TimeDistributed equivalent)\n",
    "        x = self.relu(self.fc_input(x))  # (batch, seq_len, x_dims)\n",
    "        \n",
    "        # LSTM\n",
    "        lstm_out, (h_n, c_n) = self.lstm(x)\n",
    "        # lstm_out: (batch, seq_len, x_dims) - all timestep outputs\n",
    "        # h_n: (n_layers, batch, x_dims) - last hidden state for each layer\n",
    "        \n",
    "        # Use the last hidden state from the last layer\n",
    "        last_hidden = h_n[-1]  # (batch, x_dims)\n",
    "        \n",
    "        # Output layer\n",
    "        out = self.fc_output(last_hidden)  # (batch, 1)\n",
    "        \n",
    "        return out\n",
    "\n",
    "def build_encoder():\n",
    "    model = Encoder(input_dim, x_dims, n_rnn_layers, dropout=0.2)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_encoder()\n",
    "model = model.to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test if model works\n",
    "tmp_x_batch = torch.randn(1, 32, input_dim).to(device)\n",
    "output = model(tmp_x_batch)\n",
    "print(output.shape)  # (1, 1)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16\n",
    "epochs = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert numpy arrays to PyTorch tensors\n",
    "x_train_tensor = torch.FloatTensor(x_train)\n",
    "y_train_tensor = torch.FloatTensor(y_train)\n",
    "x_test_tensor = torch.FloatTensor(x_test)\n",
    "y_test_tensor = torch.FloatTensor(y_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_dataset = TensorDataset(x_train_tensor, y_train_tensor)\n",
    "test_dataset = TensorDataset(x_test_tensor, y_test_tensor)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Loss function and optimizer\n",
    "criterion = nn.L1Loss()  # Mean Absolute Error\n",
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, train_loader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_x, batch_y in train_loader:\n",
    "        batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item() * batch_x.size(0)\n",
    "    \n",
    "    return total_loss / len(train_loader.dataset)\n",
    "\n",
    "def evaluate(model, test_loader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_x, batch_y in test_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            outputs = model(batch_x)\n",
    "            loss = criterion(outputs, batch_y)\n",
    "            total_loss += loss.item() * batch_x.size(0)\n",
    "    \n",
    "    return total_loss / len(test_loader.dataset)\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss = evaluate(model, test_loader, criterion, device)\n",
    "    \n",
    "    elapsed = time.time() - start_time\n",
    "    print(f\"Epoch {epoch+1}/{epochs} - {elapsed:.1f}s - \"\n",
    "          f\"loss: {train_loss:.4f} - val_loss: {val_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for _ in range(10):\n",
    "        idx = np.random.randint(x_test.shape[0])\n",
    "        x_sample = torch.FloatTensor(x_test[idx]).unsqueeze(0).to(device)\n",
    "        y_pred = model(x_sample).cpu().numpy()[0, 0]\n",
    "\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(x_test[idx, :, 0])\n",
    "        plt.axhline(y_test[idx], color='r', label='True')\n",
    "        plt.axhline(y_pred, color='g', label='Predicted')\n",
    "        plt.legend()\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lginnotek",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
