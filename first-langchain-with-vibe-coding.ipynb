{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QKqWAEWSAdA"
      },
      "source": [
        "## 패키지 설치"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FliH4qvcqwai",
        "outputId": "e49ec3c2-6529-4f15-9f51-47ae0237568d"
      },
      "outputs": [],
      "source": [
        "!pip install langchain langchain-core langchain-google-genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyC1KL3kSHwF"
      },
      "source": [
        "## ChatGPT대신 Gemini를 사용하도록 수정한 first-langchain.py\n",
        "\n",
        "힌트: <code>(github)class-2026-lginnotek-llm/first-langchain.py</code>를 Gemini를 이용해 수정\n",
        "\n",
        "프롬프트:\n",
        "\n",
        "> [다음 코드에서 OpenAI 를 사용하고 있는데 GoogleGenerativeAI 를 사용하도록 수정해줘]\n",
        ">\n",
        "> {first-langchain.py 코드 copy & paste}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2-4Bdzl52nVn"
      },
      "outputs": [],
      "source": [
        "# 1. 라이브러리 임포트 및 API 키 설정\n",
        "import os\n",
        "# OpenAI 대신 GoogleGenerativeAI(ChatGoogleGenerativeAI)를 임포트합니다.\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4pnUAE-x25xK"
      },
      "outputs": [],
      "source": [
        "# Google API Key 설정 (환경변수 이름이 GOOGLE_API_KEY로 변경됩니다)\n",
        "os.environ[\"GOOGLE_API_KEY\"] = \"your-api-key\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "66GSceW92-H_"
      },
      "outputs": [],
      "source": [
        "# 2. 모델 초기화\n",
        "# Gemini 모델을 사용하기 위해 ChatGoogleGenerativeAI를 초기화합니다.\n",
        "# model 매개변수에 'gemini-1.5-flash' 또는 'gemini-pro' 등을 지정합니다.\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-flash-lite-latest\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# 3. 프롬프트 템플릿 생성 (기존과 동일)\n",
        "template = \"{topic}에 대해 배울 수 있는 좋은 책 3권을 추천해줘.\"\n",
        "prompt_template = PromptTemplate.from_template(template)\n",
        "\n",
        "# 4. LCEL을 사용한 체인 구성 (기존과 동일)\n",
        "# 프롬프트 -> LLM -> 문자열 출력 파서\n",
        "chain = prompt_template | llm | StrOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qihDjvFZqmwz",
        "outputId": "ed5b8278-1e6c-4208-de18-c9c3da28ebfb"
      },
      "outputs": [],
      "source": [
        "# 5. 체인 실행 (기존과 동일)\n",
        "response = chain.invoke({\"topic\": \"인공지능\"})\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7igqeJ9qSx53"
      },
      "source": [
        "## 최종 출력을 JSON형식으로 포메팅하여 출력하도록 수정\n",
        "\n",
        "프롬프트:\n",
        "> llm 의 출력을 포메팅해서 JSON 형식으로 만들고 싶어. 추천한 3권에 대해 키 \"title\", \"author\", \"reason_of_recommendation\", \"key_features\" 라고하고 각각, 제목, 저자, 추천이유, 특징을 값으로 넣어서 포메팅해줘."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YSebKexETAHi"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "from langchain_core.prompts import PromptTemplate\n",
        "from langchain_core.output_parsers import JsonOutputParser, StrOutputParser"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jmLdFojHS5k3"
      },
      "outputs": [],
      "source": [
        "# 1. 모델 초기화\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-flash-lite-latest\",\n",
        "    temperature=0.7\n",
        ")\n",
        "\n",
        "# 2. 프롬프트 템플릿 작성\n",
        "# Pydantic 대신 프롬프트 안에 직접 JSON 예시를 보여줍니다.\n",
        "# 주의: PromptTemplate 안에서 JSON의 중괄호 {}를 표현하려면 {{ }}로 두 번 감싸야 합니다.\n",
        "template = \"\"\"\n",
        "{topic}에 대해 배울 수 있는 좋은 책 3권을 추천해줘.\n",
        "\n",
        "반드시 아래와 같은 JSON 리스트 형식으로만 답변해줘 (다른 텍스트나 설명은 제외):\n",
        "\n",
        "```json\n",
        "[\n",
        "  {{\n",
        "    \"title\": \"책 제목\",\n",
        "    \"author\": \"저자\",\n",
        "    \"reason_of_recommendation\": \"추천 이유\",\n",
        "    \"key_features\": \"특징\"\n",
        "  }},\n",
        "  ...\n",
        "]\n",
        "```\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate.from_template(template)\n",
        "\n",
        "# 3. 파서 설정\n",
        "# Pydantic 객체 없이 기본 파서만 사용합니다.\n",
        "# 이 파서는 모델의 출력에서 JSON 부분만 추출하여 파이썬 딕셔너리/리스트로 변환해줍니다.\n",
        "parser = JsonOutputParser()\n",
        "\n",
        "# 4. 체인 구성\n",
        "chain = prompt_template | llm | JsonOutputParser()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TxAJhja8VI4d",
        "outputId": "5b71644c-1556-4052-8a5b-745c00a705f0"
      },
      "outputs": [],
      "source": [
        "# 5. 실행\n",
        "response = chain.invoke({\"topic\": \"인공지능\"})\n",
        "\n",
        "# 결과 확인 (이미 딕셔너리 리스트 형태로 변환되어 있음)\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7_dYWCddVKLJ",
        "outputId": "f904383b-b7b5-4921-87e8-f721b9ee640e"
      },
      "outputs": [],
      "source": [
        "# 보기 좋게 출력하고 싶다면\n",
        "import json\n",
        "print(json.dumps(response, indent=2, ensure_ascii=False))"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
